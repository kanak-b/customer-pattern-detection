{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a0b030e-868b-4254-9369-fe57a8d03f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded CSV into Spark: /Volumes/workspace/default/vol/transactions.csv\n",
      "✅ Loaded CSV into Spark: /Volumes/workspace/default/vol/CustomerImportance.csv\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "|step|     customer|age|gender|zipcodeOri|     merchant|zipMerchant|           category|amount|fraud|\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "|   0|'C1093826151'|'4'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  4.55|    0|\n",
      "|   0| 'C352968107'|'2'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 39.68|    0|\n",
      "|   0|'C2054744914'|'4'|   'F'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 26.89|    0|\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "only showing top 3 rows\n",
      "+-------------+-------------+------+-------------------+-----+\n",
      "|       Source|       Target|Weight|          typeTrans|fraud|\n",
      "+-------------+-------------+------+-------------------+-----+\n",
      "|'C1093826151'| 'M348934600'|  4.55|'es_transportation'|    0|\n",
      "| 'C352968107'| 'M348934600'| 39.68|'es_transportation'|    0|\n",
      "|'C2054744914'|'M1823072687'| 26.89|'es_transportation'|    0|\n",
      "+-------------+-------------+------+-------------------+-----+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "def load_csv_to_spark(path: str):\n",
    "    \"\"\"Load CSV file into Spark DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        print(f\"✅ Loaded CSV into Spark: {path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load CSV {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Define files ===\n",
    "files_to_download = {\n",
    "    \"transactions\": {\n",
    "        \"file_id\": \"1AGXVlDhbMbhoGXDJG0IThnqz86Qy3hqb\",\n",
    "        \"path\": \"/Volumes/workspace/default/vol/transactions.csv\"\n",
    "    },\n",
    "    \"cust_imp\": {\n",
    "        \"file_id\": \"1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\",\n",
    "        \"path\": \"/Volumes/workspace/default/vol/CustomerImportance.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Download and Load ===\n",
    "dataframes = {}\n",
    "for name, info in files_to_download.items():\n",
    "    dataframes[name] = load_csv_to_spark(info[\"path\"])\n",
    "\n",
    "transactions = dataframes[\"transactions\"]\n",
    "cust_imp = dataframes[\"cust_imp\"]\n",
    "\n",
    "transactions.show(3)\n",
    "cust_imp.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a73284-87e9-4588-b814-0e637f3d1d81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "ist_time = F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\")\n",
    "\n",
    "def get_current_ist_time():\n",
    "    return datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ---------------------\n",
    "#  Pattern 1: UPGRADE\n",
    "# ---------------------\n",
    "def detect_pat1_upgrade_customers(transactions_df, cust_imp_df, y_start_time, detection_time):\n",
    "    transactions_clean = transactions.select(\n",
    "        F.col(\"customer\").alias(\"customerId\"),\n",
    "        F.col(\"merchant\").alias(\"merchantId\"),\n",
    "        F.col(\"category\").alias(\"transactionType\")\n",
    "    )\n",
    "\n",
    "    cust_imp_clean = cust_imp.select(\n",
    "        F.col(\"Source\").alias(\"customerId\"),\n",
    "        F.col(\"Target\").alias(\"merchantId\"),\n",
    "        F.col(\"Weight\").cast(\"double\").alias(\"weight\"),\n",
    "        F.col(\"typeTrans\").alias(\"transactionType\")\n",
    "    )\n",
    "\n",
    "    # Step 1:\n",
    "    # Total transactions per merchant\n",
    "    merchant_txn_counts = transactions_clean.groupBy(\"merchantId\").agg(\n",
    "        F.count(\"*\").alias(\"total_txns\")\n",
    "    )\n",
    "\n",
    "    # filter merchants with ≥ 50k transactions\n",
    "    eligible_merchants = merchant_txn_counts.filter(\"total_txns >= 50000\")\n",
    "\n",
    "    # Join to transactions to keep only eligible merchants\n",
    "    filtered_txns = transactions_clean.join(\n",
    "        eligible_merchants.select(\"merchantId\"), on=\"merchantId\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Step 2:\n",
    "    # Count of transactions per customer per merchant\n",
    "    cust_txn_counts = filtered_txns.groupBy(\"merchantId\", \"customerId\").agg(\n",
    "        F.count(\"*\").alias(\"txn_count\")\n",
    "    )\n",
    "\n",
    "    # Window for percentile calculation per merchant\n",
    "    txn_window = Window.partitionBy(\"merchantId\").orderBy(F.desc(\"txn_count\"))\n",
    "\n",
    "    # Add raw percentile rank\n",
    "    cust_txn_counts = cust_txn_counts.withColumn(\n",
    "        \"txn_percentile_rank_raw\",\n",
    "        F.percent_rank().over(txn_window)\n",
    "    )\n",
    "\n",
    "    # Round to 4 decimal places to remove scientific notation\n",
    "    cust_txn_counts = cust_txn_counts.withColumn(\n",
    "        \"txn_percentile_rank\", F.round(\"txn_percentile_rank_raw\", 4)\n",
    "    ).drop(\"txn_percentile_rank_raw\")\n",
    "\n",
    "    # Top 10% customers by txn count\n",
    "    top_txn_customers = cust_txn_counts.filter(F.col(\"txn_percentile_rank\") <= 0.1)\n",
    "    # Step 3:\n",
    "    # Join with cust_imp to get average weight\n",
    "    joined = top_txn_customers.join(\n",
    "        cust_imp_clean,\n",
    "        on=[\"merchantId\", \"customerId\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    avg_weight_df = joined.groupBy(\"merchantId\", \"customerId\").agg(\n",
    "        F.avg(\"weight\").alias(\"avg_weight\")\n",
    "    )\n",
    "\n",
    "    # Window to rank avg_weight per merchant\n",
    "    weight_window = Window.partitionBy(\"merchantId\").orderBy(\"avg_weight\")\n",
    "\n",
    "    avg_weight_df = avg_weight_df.withColumn(\n",
    "        \"weight_percentile_rank\",\n",
    "        F.percent_rank().over(weight_window)\n",
    "    )\n",
    "\n",
    "    # Bottom 10% by weight\n",
    "    final_pat1 = avg_weight_df.filter(\"weight_percentile_rank <= 0.1\")\n",
    "    result_pat1 = final_pat1.withColumn(\"patternId\", F.lit(\"PatId1\")) \\\n",
    "        .withColumn(\"actionType\", F.lit(\"UPGRADE\")) \\\n",
    "        .withColumn(\"YStartTime\", ist_time) \\\n",
    "        .withColumn(\"detectionTime\", ist_time) \\\n",
    "        .withColumn(\"customerName\", F.col(\"customerId\")) \\\n",
    "        .select(\n",
    "            \"YStartTime\", \"detectionTime\", \"patternId\", \"actionType\",\n",
    "            \"customerName\", \"merchantId\"\n",
    "        )\n",
    "\n",
    "    return result_pat1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Pattern 2: CHILD\n",
    "# ---------------------\n",
    "def detect_pat2_child_customers(transactions_df, y_start_time, detection_time):\n",
    "    pat2_df = transactions_df.groupBy(\"customer\", \"merchant\") \\\n",
    "                .agg(\n",
    "                    F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "                    F.count(\"*\").alias(\"txn_count\")\n",
    "                ).filter(\"avg_amount < 23 AND txn_count >= 80\")\n",
    "\n",
    "    result_df = pat2_df.selectExpr(\n",
    "        f\"'{y_start_time}' as YStartTime\",\n",
    "        f\"'{detection_time}' as detectionTime\",\n",
    "        \"'PatId2' as patternId\",\n",
    "        \"'CHILD' as actionType\",\n",
    "        \"customer as customerName\",\n",
    "        \"merchant as merchantId\"\n",
    "    )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# ---------------------\n",
    "# Pattern 3: DEI-NEEDED\n",
    "# ---------------------\n",
    "def detect_pat3_dei_needed_merchants(transactions_df, y_start_time, detection_time):\n",
    "    gender_df = transactions.select(\n",
    "        F.expr(\"substring(merchant, 2, length(merchant) - 2)\").alias(\"merchantId\"),  # remove quotes\n",
    "        F.expr(\"substring(customer, 2, length(customer) - 2)\").alias(\"customerId\"),  # remove quotes\n",
    "        F.expr(\"substring(gender, 2, 1)\").alias(\"gender\")  # 'F' → F, 'M' → M\n",
    "    )\n",
    "\n",
    "    # Count distinct genders per customer per merchant\n",
    "    gender_clean_check = gender_df.groupBy(\"merchantId\", \"customerId\") \\\n",
    "        .agg(F.countDistinct(\"gender\").alias(\"gender_type_count\"))\n",
    "\n",
    "    # Keep only clean records (1 gender only per customer per merchant)\n",
    "    clean_customers = gender_clean_check.filter(\"gender_type_count = 1\") \\\n",
    "        .select(\"merchantId\", \"customerId\")\n",
    "\n",
    "    # Join back to filter ambiguous entries\n",
    "    gender_cleaned = gender_df.join(clean_customers, on=[\"merchantId\", \"customerId\"], how=\"inner\")\n",
    "    # Remove duplicate customer-merchant-gender combos\n",
    "    unique_pairs = gender_cleaned.select(\"merchantId\", \"customerId\", \"gender\").distinct()\n",
    "\n",
    "    # Pivot to get gender counts\n",
    "    gender_counts = unique_pairs.groupBy(\"merchantId\") \\\n",
    "        .pivot(\"gender\", [\"F\", \"M\"]).count().fillna(0)\n",
    "    dei_merchants = gender_counts.filter(\n",
    "        (F.col(\"F\") > 100) & (F.col(\"F\") < F.col(\"M\"))\n",
    "    )\n",
    "    pat3_df = dei_merchants \\\n",
    "        .withColumn(\"patternId\", F.lit(\"PatId3\")) \\\n",
    "        .withColumn(\"actionType\", F.lit(\"DEI-NEEDED\")) \\\n",
    "        .withColumn(\"YStartTime\", F.lit(y_start_time)) \\\n",
    "        .withColumn(\"detectionTime\", F.lit(detection_time)) \\\n",
    "        .withColumn(\"customerName\", F.lit(\"\")) \\\n",
    "        .select(\n",
    "            \"YStartTime\", \"detectionTime\", \"patternId\", \"actionType\",\n",
    "            \"customerName\", \"merchantId\"\n",
    "        )\n",
    "\n",
    "    return pat3_df\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "#  Master detection runner\n",
    "# --------------------------\n",
    "def run_pattern_detections(transactions_df, cust_imp_df):\n",
    "    y_start_time = get_current_ist_time()\n",
    "    detection_time = y_start_time\n",
    "\n",
    "    pat1_df = detect_pat1_upgrade_customers(transactions_df, cust_imp_df, y_start_time, detection_time)\n",
    "    pat2_df = detect_pat2_child_customers(transactions_df, y_start_time, detection_time)\n",
    "    pat3_df = detect_pat3_dei_needed_merchants(transactions_df, y_start_time, detection_time)\n",
    "\n",
    "    return pat1_df.unionByName(pat2_df).unionByName(pat3_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ca70fd-20c9-43d2-99d6-6ffb162f4495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = run_pattern_detections(transactions, cust_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3466bc6-522c-481d-81ad-6bf9aa659eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a98215-20ec-4aaf-8042-659c37be6062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+----------+-------------+-------------+\n",
      "|          YStartTime|       detectionTime|patternId|actionType| customerName|   merchantId|\n",
      "+--------------------+--------------------+---------+----------+-------------+-------------+\n",
      "|2025-07-12 19:11:...|2025-07-12 19:11:...|   PatId1|   UPGRADE|'C1214229415'|'M1823072687'|\n",
      "|2025-07-12 19:11:...|2025-07-12 19:11:...|   PatId1|   UPGRADE|'C2035771335'|'M1823072687'|\n",
      "|2025-07-12 19:11:...|2025-07-12 19:11:...|   PatId1|   UPGRADE|  'C15276068'|'M1823072687'|\n",
      "|2025-07-12 19:11:...|2025-07-12 19:11:...|   PatId1|   UPGRADE| 'C861002792'|'M1823072687'|\n",
      "|2025-07-12 19:11:...|2025-07-12 19:11:...|   PatId1|   UPGRADE| 'C566598245'|'M1823072687'|\n",
      "+--------------------+--------------------+---------+----------+-------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1e8594-4ce2-4cae-8774-4e5b6b0f1d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3, time, pandas as pd, os,io\n",
    "from threading import Thread\n",
    "from pyspark.sql.functions import monotonically_increasing_id, floor\n",
    "\n",
    "# Common AWS/S3 setup\n",
    "\n",
    "bucket = \"pattern-detection-kanakb\"\n",
    "prefix = \"transaction_chunks\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "def producer():\n",
    "    print(\"🚀 Stream Producer started...\")\n",
    "\n",
    "    full_df = transactions.toPandas() \n",
    "\n",
    "    for i in range(0, len(full_df), chunk_size):\n",
    "        chunk = full_df[i:i+chunk_size]\n",
    "\n",
    "        # Save locally\n",
    "        local_path = f\"/tmp/chunk_{i}.csv\"\n",
    "        chunk.to_csv(local_path, index=False)\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_key = f\"{prefix}/chunk_{i}.csv\"\n",
    "        s3.upload_file(local_path, bucket, s3_key)\n",
    "\n",
    "        print(f\"✅ Uploaded: s3://{bucket}/{s3_key}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "def consumer():\n",
    "    print(\"👂 Consumer started...\")\n",
    "\n",
    "    expected_chunk = 0\n",
    "    total_rows = 0\n",
    "    detection_file_index = 0\n",
    "    detection_buffer_df = pd.DataFrame()\n",
    "\n",
    "    while True:\n",
    "        s3_key = f\"{prefix}/chunk_{expected_chunk}.csv\"\n",
    "        try:\n",
    "            # Step 1: Read chunk from S3\n",
    "            response = s3.get_object(Bucket=bucket, Key=s3_key)\n",
    "            content = response['Body'].read().decode('utf-8')\n",
    "            chunk_df = pd.read_csv(io.StringIO(content))\n",
    "\n",
    "            print(f\"📥 Processed: {s3_key}, rows = {len(chunk_df)}\")\n",
    "            total_rows += len(chunk_df)\n",
    "\n",
    "            # Step 2: Convert to Spark DataFrame\n",
    "            spark_df = spark.createDataFrame(chunk_df)\n",
    "\n",
    "            # Step 3: Run detection\n",
    "            detection_df = run_pattern_detections(spark_df, cust_imp)\n",
    "            detection_pd = detection_df.toPandas()\n",
    "\n",
    "            # Step 4: Append to buffer\n",
    "            detection_buffer_df = pd.concat([detection_buffer_df, detection_pd], ignore_index=True)\n",
    "\n",
    "            # Step 5: Write only full batches of 50\n",
    "            while len(detection_buffer_df) >= 50:\n",
    "                batch_df = detection_buffer_df.iloc[:50]\n",
    "                detection_buffer_df = detection_buffer_df.iloc[50:]\n",
    "\n",
    "                csv_buffer = io.StringIO()\n",
    "                batch_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "                detection_key = f\"{prefix.replace('transaction_chunks', 'output_detections')}/detection_{detection_file_index}.csv\"\n",
    "                s3.put_object(Bucket=bucket, Key=detection_key, Body=csv_buffer.getvalue())\n",
    "                print(f\"✅ Wrote detection batch: {detection_key}\")\n",
    "                detection_file_index += 1\n",
    "\n",
    "            expected_chunk += chunk_size\n",
    "        except s3.exceptions.NoSuchKey:\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {s3_key}: {e}\")\n",
    "\n",
    "    # Step 6: Flush remaining rows < 50 (only at the end)\n",
    "    if not detection_buffer_df.empty:\n",
    "        csv_buffer = io.StringIO()\n",
    "        detection_buffer_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        detection_key = f\"{prefix.replace('transaction_chunks', 'output_detections')}/detection_{detection_file_index}.csv\"\n",
    "        s3.put_object(Bucket=bucket, Key=detection_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"🟡 Final flush: {detection_key} with {len(detection_buffer_df)} rows\")\n",
    "\n",
    "    print(f\"✅ Consumer finished processing {total_rows} rows.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aeb3335-1c3a-4881-a7be-9f0522ec41ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Stream Producer started...\n",
      "👂 Consumer started...\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_0.csv\n",
      "📥 Processed: transaction_chunks/chunk_0.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_10000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_20000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_0.csv\n",
      "📥 Processed: transaction_chunks/chunk_10000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_30000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_40000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_1.csv\n",
      "✅ Wrote detection batch: output_detections/detection_2.csv\n",
      "📥 Processed: transaction_chunks/chunk_20000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_50000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_60000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_3.csv\n",
      "📥 Processed: transaction_chunks/chunk_30000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_70000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_80000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_4.csv\n",
      "✅ Wrote detection batch: output_detections/detection_5.csv\n",
      "📥 Processed: transaction_chunks/chunk_40000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_90000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_100000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_6.csv\n",
      "✅ Wrote detection batch: output_detections/detection_7.csv\n",
      "📥 Processed: transaction_chunks/chunk_50000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_110000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_120000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_8.csv\n",
      "📥 Processed: transaction_chunks/chunk_60000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_130000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_140000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_9.csv\n",
      "✅ Wrote detection batch: output_detections/detection_10.csv\n",
      "📥 Processed: transaction_chunks/chunk_70000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_150000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_160000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_11.csv\n",
      "📥 Processed: transaction_chunks/chunk_80000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_170000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_180000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_12.csv\n",
      "✅ Wrote detection batch: output_detections/detection_13.csv\n",
      "📥 Processed: transaction_chunks/chunk_90000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_190000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_200000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_210000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_220000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_230000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_240000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_250000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_14.csv\n",
      "✅ Wrote detection batch: output_detections/detection_15.csv\n",
      "📥 Processed: transaction_chunks/chunk_100000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_260000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_270000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_16.csv\n",
      "📥 Processed: transaction_chunks/chunk_110000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_280000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_290000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_17.csv\n",
      "✅ Wrote detection batch: output_detections/detection_18.csv\n",
      "📥 Processed: transaction_chunks/chunk_120000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_300000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_310000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_19.csv\n",
      "✅ Wrote detection batch: output_detections/detection_20.csv\n",
      "📥 Processed: transaction_chunks/chunk_130000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_320000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_330000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_21.csv\n",
      "📥 Processed: transaction_chunks/chunk_140000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_340000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_22.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_350000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_23.csv\n",
      "📥 Processed: transaction_chunks/chunk_150000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_360000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_24.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_370000.csv\n",
      "📥 Processed: transaction_chunks/chunk_160000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_380000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_390000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_25.csv\n",
      "✅ Wrote detection batch: output_detections/detection_26.csv\n",
      "📥 Processed: transaction_chunks/chunk_170000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_400000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_27.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_410000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_28.csv\n",
      "📥 Processed: transaction_chunks/chunk_180000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_420000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_29.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_430000.csv\n",
      "📥 Processed: transaction_chunks/chunk_190000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_440000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_30.csv\n",
      "✅ Wrote detection batch: output_detections/detection_31.csv\n",
      "📥 Processed: transaction_chunks/chunk_200000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_450000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_460000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_32.csv\n",
      "✅ Wrote detection batch: output_detections/detection_33.csv\n",
      "📥 Processed: transaction_chunks/chunk_210000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_470000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_480000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_34.csv\n",
      "📥 Processed: transaction_chunks/chunk_220000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_490000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_500000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_35.csv\n",
      "✅ Wrote detection batch: output_detections/detection_36.csv\n",
      "📥 Processed: transaction_chunks/chunk_230000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_510000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_520000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_37.csv\n",
      "📥 Processed: transaction_chunks/chunk_240000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_530000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_540000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_550000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_38.csv\n",
      "✅ Wrote detection batch: output_detections/detection_39.csv\n",
      "📥 Processed: transaction_chunks/chunk_250000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_560000.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_570000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_40.csv\n",
      "✅ Wrote detection batch: output_detections/detection_41.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_580000.csv\n",
      "📥 Processed: transaction_chunks/chunk_260000.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_590000.csv\n",
      "✅ Wrote detection batch: output_detections/detection_42.csv\n",
      "📥 Processed: transaction_chunks/chunk_270000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_43.csv\n",
      "✅ Wrote detection batch: output_detections/detection_44.csv\n",
      "📥 Processed: transaction_chunks/chunk_280000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_45.csv\n",
      "📥 Processed: transaction_chunks/chunk_290000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_46.csv\n",
      "✅ Wrote detection batch: output_detections/detection_47.csv\n",
      "📥 Processed: transaction_chunks/chunk_300000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_48.csv\n",
      "✅ Wrote detection batch: output_detections/detection_49.csv\n",
      "📥 Processed: transaction_chunks/chunk_310000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_50.csv\n",
      "📥 Processed: transaction_chunks/chunk_320000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_51.csv\n",
      "✅ Wrote detection batch: output_detections/detection_52.csv\n",
      "📥 Processed: transaction_chunks/chunk_330000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_53.csv\n",
      "✅ Wrote detection batch: output_detections/detection_54.csv\n",
      "📥 Processed: transaction_chunks/chunk_340000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_55.csv\n",
      "📥 Processed: transaction_chunks/chunk_350000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_56.csv\n",
      "✅ Wrote detection batch: output_detections/detection_57.csv\n",
      "📥 Processed: transaction_chunks/chunk_360000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_58.csv\n",
      "📥 Processed: transaction_chunks/chunk_370000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_59.csv\n",
      "✅ Wrote detection batch: output_detections/detection_60.csv\n",
      "📥 Processed: transaction_chunks/chunk_380000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_61.csv\n",
      "✅ Wrote detection batch: output_detections/detection_62.csv\n",
      "📥 Processed: transaction_chunks/chunk_390000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_63.csv\n",
      "📥 Processed: transaction_chunks/chunk_400000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_64.csv\n",
      "✅ Wrote detection batch: output_detections/detection_65.csv\n",
      "📥 Processed: transaction_chunks/chunk_410000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_66.csv\n",
      "✅ Wrote detection batch: output_detections/detection_67.csv\n",
      "📥 Processed: transaction_chunks/chunk_420000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_68.csv\n",
      "📥 Processed: transaction_chunks/chunk_430000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_69.csv\n",
      "✅ Wrote detection batch: output_detections/detection_70.csv\n",
      "📥 Processed: transaction_chunks/chunk_440000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_71.csv\n",
      "📥 Processed: transaction_chunks/chunk_450000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_72.csv\n",
      "✅ Wrote detection batch: output_detections/detection_73.csv\n",
      "📥 Processed: transaction_chunks/chunk_460000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_74.csv\n",
      "✅ Wrote detection batch: output_detections/detection_75.csv\n",
      "📥 Processed: transaction_chunks/chunk_470000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_76.csv\n",
      "📥 Processed: transaction_chunks/chunk_480000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_77.csv\n",
      "✅ Wrote detection batch: output_detections/detection_78.csv\n",
      "📥 Processed: transaction_chunks/chunk_490000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_79.csv\n",
      "✅ Wrote detection batch: output_detections/detection_80.csv\n",
      "📥 Processed: transaction_chunks/chunk_500000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_81.csv\n",
      "📥 Processed: transaction_chunks/chunk_510000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_82.csv\n",
      "✅ Wrote detection batch: output_detections/detection_83.csv\n",
      "📥 Processed: transaction_chunks/chunk_520000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_84.csv\n",
      "📥 Processed: transaction_chunks/chunk_530000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_85.csv\n",
      "✅ Wrote detection batch: output_detections/detection_86.csv\n",
      "📥 Processed: transaction_chunks/chunk_540000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_87.csv\n",
      "✅ Wrote detection batch: output_detections/detection_88.csv\n",
      "📥 Processed: transaction_chunks/chunk_550000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_89.csv\n",
      "📥 Processed: transaction_chunks/chunk_560000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_90.csv\n",
      "✅ Wrote detection batch: output_detections/detection_91.csv\n",
      "📥 Processed: transaction_chunks/chunk_570000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_92.csv\n",
      "📥 Processed: transaction_chunks/chunk_580000.csv, rows = 10000\n",
      "✅ Wrote detection batch: output_detections/detection_93.csv\n",
      "✅ Wrote detection batch: output_detections/detection_94.csv\n",
      "📥 Processed: transaction_chunks/chunk_590000.csv, rows = 4643\n",
      "✅ Wrote detection batch: output_detections/detection_95.csv\n",
      "✅ Wrote detection batch: output_detections/detection_96.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:132)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:132)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:129)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:129)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:715)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:466)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:757)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:83)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:83)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:735)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:926)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:952)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:951)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:1006)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:777)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1037)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:957)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:522)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:806)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:806)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:769)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:751)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:52)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:415)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "producer_thread = Thread(target=producer)\n",
    "consumer_thread = Thread(target=consumer)\n",
    "\n",
    "# Start threads\n",
    "producer_thread.start()\n",
    "consumer_thread.start()\n",
    "\n",
    "# Wait for both to finish\n",
    "producer_thread.join()\n",
    "consumer_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecbb3b63-aff0-45f8-b9ef-fd314bbe0ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pattern-detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
