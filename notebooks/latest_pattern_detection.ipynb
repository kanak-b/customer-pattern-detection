{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b15852-0526-4b43-bb94-42a79ee6d538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded CSV into Spark: /Volumes/workspace/default/vol/transactions.csv\n",
      "✅ Loaded CSV into Spark: /Volumes/workspace/default/vol/CustomerImportance.csv\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "|step|     customer|age|gender|zipcodeOri|     merchant|zipMerchant|           category|amount|fraud|\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "|   0|'C1093826151'|'4'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  4.55|    0|\n",
      "|   0| 'C352968107'|'2'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 39.68|    0|\n",
      "|   0|'C2054744914'|'4'|   'F'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 26.89|    0|\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "only showing top 3 rows\n",
      "+-------------+-------------+------+-------------------+-----+\n",
      "|       Source|       Target|Weight|          typeTrans|fraud|\n",
      "+-------------+-------------+------+-------------------+-----+\n",
      "|'C1093826151'| 'M348934600'|  4.55|'es_transportation'|    0|\n",
      "| 'C352968107'| 'M348934600'| 39.68|'es_transportation'|    0|\n",
      "|'C2054744914'|'M1823072687'| 26.89|'es_transportation'|    0|\n",
      "+-------------+-------------+------+-------------------+-----+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "def load_csv_to_spark(path: str):\n",
    "    \"\"\"Load CSV file into Spark DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(path, header=True, inferSchema=True)\n",
    "        print(f\"✅ Loaded CSV into Spark: {path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load CSV {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# === Define files ===\n",
    "files_to_download = {\n",
    "    \"transactions\": {\n",
    "        \"file_id\": \"1AGXVlDhbMbhoGXDJG0IThnqz86Qy3hqb\",\n",
    "        \"path\": \"/Volumes/workspace/default/vol/transactions.csv\"\n",
    "    },\n",
    "    \"cust_imp\": {\n",
    "        \"file_id\": \"1abe9EkM_uf2F2hjEkbhMBG9Mf2dFE4Wo\",\n",
    "        \"path\": \"/Volumes/workspace/default/vol/CustomerImportance.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Download and Load ===\n",
    "dataframes = {}\n",
    "for name, info in files_to_download.items():\n",
    "    dataframes[name] = load_csv_to_spark(info[\"path\"])\n",
    "\n",
    "transactions = dataframes[\"transactions\"]\n",
    "cust_imp = dataframes[\"cust_imp\"]\n",
    "\n",
    "transactions.show(3)\n",
    "cust_imp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9360b8-7422-49c2-b89b-06adc6bc06c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "ist_time = F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\")\n",
    "\n",
    "def get_current_ist_time():\n",
    "    return datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ---------------------\n",
    "#  Pattern 1: UPGRADE\n",
    "# ---------------------\n",
    "def detect_pat1_upgrade_customers(transactions_df, cust_imp_df, y_start_time, detection_time):\n",
    "    transactions_clean = transactions.select(\n",
    "        F.col(\"customer\").alias(\"customerId\"),\n",
    "        F.col(\"merchant\").alias(\"merchantId\"),\n",
    "        F.col(\"category\").alias(\"transactionType\")\n",
    "    )\n",
    "\n",
    "    cust_imp_clean = cust_imp.select(\n",
    "        F.col(\"Source\").alias(\"customerId\"),\n",
    "        F.col(\"Target\").alias(\"merchantId\"),\n",
    "        F.col(\"Weight\").cast(\"double\").alias(\"weight\"),\n",
    "        F.col(\"typeTrans\").alias(\"transactionType\")\n",
    "    )\n",
    "\n",
    "    # Step 1:\n",
    "    # Total transactions per merchant\n",
    "    merchant_txn_counts = transactions_clean.groupBy(\"merchantId\").agg(\n",
    "        F.count(\"*\").alias(\"total_txns\")\n",
    "    )\n",
    "\n",
    "    # filter merchants with ≥ 50k transactions\n",
    "    eligible_merchants = merchant_txn_counts.filter(\"total_txns >= 50000\")\n",
    "\n",
    "    # Join to transactions to keep only eligible merchants\n",
    "    filtered_txns = transactions_clean.join(\n",
    "        eligible_merchants.select(\"merchantId\"), on=\"merchantId\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Step 2:\n",
    "    # Count of transactions per customer per merchant\n",
    "    cust_txn_counts = filtered_txns.groupBy(\"merchantId\", \"customerId\").agg(\n",
    "        F.count(\"*\").alias(\"txn_count\")\n",
    "    )\n",
    "\n",
    "    # Window for percentile calculation per merchant\n",
    "    txn_window = Window.partitionBy(\"merchantId\").orderBy(F.desc(\"txn_count\"))\n",
    "\n",
    "    # Add raw percentile rank\n",
    "    cust_txn_counts = cust_txn_counts.withColumn(\n",
    "        \"txn_percentile_rank_raw\",\n",
    "        F.percent_rank().over(txn_window)\n",
    "    )\n",
    "\n",
    "    # Round to 4 decimal places to remove scientific notation\n",
    "    cust_txn_counts = cust_txn_counts.withColumn(\n",
    "        \"txn_percentile_rank\", F.round(\"txn_percentile_rank_raw\", 4)\n",
    "    ).drop(\"txn_percentile_rank_raw\")\n",
    "\n",
    "    # Top 10% customers by txn count\n",
    "    top_txn_customers = cust_txn_counts.filter(F.col(\"txn_percentile_rank\") <= 0.1)\n",
    "    # Step 3:\n",
    "    # Join with cust_imp to get average weight\n",
    "    joined = top_txn_customers.join(\n",
    "        cust_imp_clean,\n",
    "        on=[\"merchantId\", \"customerId\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    avg_weight_df = joined.groupBy(\"merchantId\", \"customerId\").agg(\n",
    "        F.avg(\"weight\").alias(\"avg_weight\")\n",
    "    )\n",
    "\n",
    "    # Window to rank avg_weight per merchant\n",
    "    weight_window = Window.partitionBy(\"merchantId\").orderBy(\"avg_weight\")\n",
    "\n",
    "    avg_weight_df = avg_weight_df.withColumn(\n",
    "        \"weight_percentile_rank\",\n",
    "        F.percent_rank().over(weight_window)\n",
    "    )\n",
    "\n",
    "    # Bottom 10% by weight\n",
    "    final_pat1 = avg_weight_df.filter(\"weight_percentile_rank <= 0.1\")\n",
    "    result_pat1 = final_pat1.withColumn(\"patternId\", F.lit(\"PatId1\")) \\\n",
    "        .withColumn(\"actionType\", F.lit(\"UPGRADE\")) \\\n",
    "        .withColumn(\"YStartTime\", ist_time) \\\n",
    "        .withColumn(\"detectionTime\", ist_time) \\\n",
    "        .withColumn(\"customerName\", F.col(\"customerId\")) \\\n",
    "        .select(\n",
    "            \"YStartTime\", \"detectionTime\", \"patternId\", \"actionType\",\n",
    "            \"customerName\", \"merchantId\"\n",
    "        )\n",
    "\n",
    "    return result_pat1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------\n",
    "#  Pattern 2: CHILD\n",
    "# ---------------------\n",
    "def detect_pat2_child_customers(transactions_df, y_start_time, detection_time):\n",
    "    pat2_df = transactions_df.groupBy(\"customer\", \"merchant\") \\\n",
    "                .agg(\n",
    "                    F.avg(\"amount\").alias(\"avg_amount\"),\n",
    "                    F.count(\"*\").alias(\"txn_count\")\n",
    "                ).filter(\"avg_amount < 23 AND txn_count >= 80\")\n",
    "\n",
    "    result_df = pat2_df.selectExpr(\n",
    "        f\"'{y_start_time}' as YStartTime\",\n",
    "        f\"'{detection_time}' as detectionTime\",\n",
    "        \"'PatId2' as patternId\",\n",
    "        \"'CHILD' as actionType\",\n",
    "        \"customer as customerName\",\n",
    "        \"merchant as merchantId\"\n",
    "    )\n",
    "\n",
    "    return result_df\n",
    "\n",
    "# ---------------------\n",
    "# Pattern 3: DEI-NEEDED\n",
    "# ---------------------\n",
    "def detect_pat3_dei_needed_merchants(transactions_df, y_start_time, detection_time):\n",
    "    gender_df = transactions.select(\n",
    "        F.expr(\"substring(merchant, 2, length(merchant) - 2)\").alias(\"merchantId\"),  # remove quotes\n",
    "        F.expr(\"substring(customer, 2, length(customer) - 2)\").alias(\"customerId\"),  # remove quotes\n",
    "        F.expr(\"substring(gender, 2, 1)\").alias(\"gender\")  # 'F' → F, 'M' → M\n",
    "    )\n",
    "\n",
    "    # Count distinct genders per customer per merchant\n",
    "    gender_clean_check = gender_df.groupBy(\"merchantId\", \"customerId\") \\\n",
    "        .agg(F.countDistinct(\"gender\").alias(\"gender_type_count\"))\n",
    "\n",
    "    # Keep only clean records (1 gender only per customer per merchant)\n",
    "    clean_customers = gender_clean_check.filter(\"gender_type_count = 1\") \\\n",
    "        .select(\"merchantId\", \"customerId\")\n",
    "\n",
    "    # Join back to filter ambiguous entries\n",
    "    gender_cleaned = gender_df.join(clean_customers, on=[\"merchantId\", \"customerId\"], how=\"inner\")\n",
    "    # Remove duplicate customer-merchant-gender combos\n",
    "    unique_pairs = gender_cleaned.select(\"merchantId\", \"customerId\", \"gender\").distinct()\n",
    "\n",
    "    # Pivot to get gender counts\n",
    "    gender_counts = unique_pairs.groupBy(\"merchantId\") \\\n",
    "        .pivot(\"gender\", [\"F\", \"M\"]).count().fillna(0)\n",
    "    dei_merchants = gender_counts.filter(\n",
    "        (F.col(\"F\") > 100) & (F.col(\"F\") < F.col(\"M\"))\n",
    "    )\n",
    "    pat3_df = dei_merchants \\\n",
    "        .withColumn(\"patternId\", F.lit(\"PatId3\")) \\\n",
    "        .withColumn(\"actionType\", F.lit(\"DEI-NEEDED\")) \\\n",
    "        .withColumn(\"YStartTime\", F.lit(y_start_time)) \\\n",
    "        .withColumn(\"detectionTime\", F.lit(detection_time)) \\\n",
    "        .withColumn(\"customerName\", F.lit(\"\")) \\\n",
    "        .select(\n",
    "            \"YStartTime\", \"detectionTime\", \"patternId\", \"actionType\",\n",
    "            \"customerName\", \"merchantId\"\n",
    "        )\n",
    "\n",
    "    return pat3_df\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "#  Master detection runner\n",
    "# --------------------------\n",
    "def run_pattern_detections(transactions_df, cust_imp_df, include_pattern2=True):\n",
    "    y_start_time = get_current_ist_time()\n",
    "    detection_time = y_start_time\n",
    "\n",
    "    pat1_df = detect_pat1_upgrade_customers(transactions_df, cust_imp_df, y_start_time, detection_time)\n",
    "    pat3_df = detect_pat3_dei_needed_merchants(transactions_df, y_start_time, detection_time)\n",
    "\n",
    "    if include_pattern2:\n",
    "        pat2_df = detect_pat2_child_customers(transactions_df, y_start_time, detection_time)\n",
    "        return pat1_df.unionByName(pat2_df).unionByName(pat3_df)\n",
    "    else:\n",
    "        return pat1_df.unionByName(pat3_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646af78b-ca32-4b9d-a984-90d3d9a526cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "main_df = pd.DataFrame()         # Global detection store\n",
    "full_txn_df = pd.DataFrame()     # To collect all chunk data for CHILD pattern\n",
    "detection_buffer_df = pd.DataFrame()  # Buffer for batched detection writes    # File naming counter\n",
    "\n",
    "bucket = \"pattern-detection-kanakb\"\n",
    "chunk_prefix = \"transaction_chunks\"\n",
    "detection_prefix = 'output_detections'  \n",
    "\n",
    "\n",
    "s3 = boto3.client(\"s3\", aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "chunk_size = 10000\n",
    "\n",
    "def read_csv_from_s3(bucket, key):\n",
    "    \"\"\"Read CSV file from S3 and return as pandas DataFrame.\"\"\"\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "\n",
    "def write_csv_to_s3(df, bucket, key):\n",
    "    \"\"\"Write a DataFrame as CSV to S3.\"\"\"\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())\n",
    "    print(f\"📤 Written to s3://{bucket}/{key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dba70e89-fe08-4822-807e-da95aa5ed405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "chunk_size = 10000\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "chunk_index = 1\n",
    "detection_file_counter = 1   \n",
    "chunk_index_lock = Lock()\n",
    "\n",
    "\n",
    "def producer():\n",
    "    print(\"🚀 Stream Producer started...\")\n",
    "\n",
    "    global chunk_index\n",
    "    full_df = transactions.toPandas()\n",
    "\n",
    "    for i in range(0, len(full_df), chunk_size):\n",
    "        chunk = full_df[i:i+chunk_size]\n",
    "\n",
    "        with chunk_index_lock:\n",
    "            current_index = chunk_index\n",
    "            chunk_index += 1\n",
    "\n",
    "        filename = f\"/tmp/{current_index}.csv\"\n",
    "\n",
    "        # Save locally\n",
    "        chunk.to_csv(filename, index=False)\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_key = f\"{chunk_prefix}/chunk_{current_index}.csv\"\n",
    "        s3.upload_file(filename, bucket, s3_key)\n",
    "        print(f\"✅ Uploaded: s3://{bucket}/{s3_key}\")\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a370103-2cef-48a3-ad09-12daf14563c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Stream Producer started...\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_1.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_2.csv\n"
     ]
    }
   ],
   "source": [
    "producer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebaa717f-28d6-4fb3-a12a-c77e8490e750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def consumer():\n",
    "    global main_df, full_txn_df, detection_buffer_df, detection_file_counter\n",
    "    print(\"👂 Consumer started...\")\n",
    "\n",
    "    expected_chunk = 1\n",
    "    total_rows = 0\n",
    "\n",
    "    while expected_chunk < 61:\n",
    "        s3_key = f\"{chunk_prefix}/chunk_{expected_chunk}.csv\"\n",
    "\n",
    "        try:\n",
    "            print(f\"🔍 Looking for: s3://{bucket}/{s3_key}\")\n",
    "            time.sleep(1)\n",
    "            chunk_df = read_csv_from_s3(bucket, s3_key)\n",
    "            print(f\"📥 Processed: chunk_{expected_chunk}.csv, rows = {len(chunk_df)}\")\n",
    "\n",
    "            total_rows += len(chunk_df)\n",
    "            full_txn_df = pd.concat([full_txn_df, chunk_df], ignore_index=True)\n",
    "\n",
    "            spark_df = spark.createDataFrame(chunk_df)\n",
    "\n",
    "            # Run detection (excluding CHILD)\n",
    "            detection_df = run_pattern_detections(spark_df, cust_imp, include_pattern2=False)\n",
    "            detection_pd = detection_df.toPandas()\n",
    "            print(f\"🔎 Detected rows: {len(detection_pd)}\")\n",
    "\n",
    "            # Remove duplicates vs main_df and buffer\n",
    "            combined_existing = pd.concat([main_df, detection_buffer_df], ignore_index=True)\n",
    "            new_unique = pd.concat([combined_existing, detection_pd], ignore_index=True)\n",
    "            new_unique = new_unique.drop_duplicates(\n",
    "                subset=[\"patternId\", \"actionType\", \"customerName\", \"merchantId\"],\n",
    "                keep=False\n",
    "            )\n",
    "\n",
    "            # Add new unique detections to buffer\n",
    "            detection_buffer_df = pd.concat([detection_buffer_df, new_unique], ignore_index=True)\n",
    "\n",
    "            # Write detection files in batches of 50\n",
    "            while len(detection_buffer_df) >= 50:\n",
    "                write_df = detection_buffer_df.iloc[:50]\n",
    "                s3_output_key = f\"{detection_prefix}/detection_{detection_file_counter}.csv\"\n",
    "                write_csv_to_s3(write_df, bucket, s3_output_key)\n",
    "\n",
    "                print(f\"📤 Detection file written: {s3_output_key}\")\n",
    "\n",
    "                detection_file_counter += 1\n",
    "                detection_buffer_df = detection_buffer_df.iloc[50:]  # remove written rows\n",
    "\n",
    "            # Merge into main_df\n",
    "            main_df = pd.concat([main_df, new_unique], ignore_index=True)\n",
    "            main_df = main_df.drop_duplicates(\n",
    "                subset=[\"patternId\", \"actionType\", \"customerName\", \"merchantId\"],\n",
    "                keep=\"first\"\n",
    "            )\n",
    "\n",
    "            print(f\"📊 Detections so far: {len(main_df)}\")\n",
    "            expected_chunk += 1\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {s3_key}: {e}\")\n",
    "\n",
    "    # Run CHILD pattern after all chunks are processed\n",
    "    print(f\"📦 Running CHILD detection on full {len(full_txn_df)} rows...\")\n",
    "    full_spark_df = spark.createDataFrame(full_txn_df)\n",
    "    y_start_time = get_current_ist_time()\n",
    "    detection_time = y_start_time\n",
    "    pat2_df = detect_pat2_child_customers(full_spark_df, y_start_time, detection_time)\n",
    "    pat2_pd = pat2_df.toPandas()\n",
    "    print(f\"Found CHILD Detections: {len(pat2_pd)}\")\n",
    "\n",
    "    detection_buffer_df = pd.concat([detection_buffer_df, pat2_pd], ignore_index=True)\n",
    "\n",
    "    # Write remaining detections in batches of 50\n",
    "    while len(detection_buffer_df) >= 50:\n",
    "        write_df = detection_buffer_df.iloc[:50]\n",
    "        s3_output_key = f\"{detection_prefix}/detection_{detection_file_counter}.csv\"\n",
    "        write_csv_to_s3(write_df, bucket, s3_output_key)\n",
    "\n",
    "        print(f\"📤 Detection file written: {s3_output_key}\")\n",
    "\n",
    "        detection_file_counter += 1\n",
    "        detection_buffer_df = detection_buffer_df.iloc[50:]\n",
    "\n",
    "    # Write any remaining detections (even if <50) to a final file\n",
    "    if not detection_buffer_df.empty:\n",
    "\n",
    "        s3_output_key = f\"{detection_prefix}/detection_{detection_file_counter}.csv\"\n",
    "        write_csv_to_s3(detection_buffer_df, bucket, s3_output_key)\n",
    "\n",
    "        print(f\"📤 Final detection file (less than 50 rows): {s3_output_key}\")\n",
    "        detection_file_counter += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14b9211-8735-42eb-9f68-f385bed32967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Stream Producer started...\n",
      "👂 Consumer started...\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_1.csv\n",
      "❌ Error processing transaction_chunks/chunk_1.csv: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_1.csv\n",
      "❌ Error processing transaction_chunks/chunk_1.csv: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_1.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_1.csv\n",
      "📥 Processed: chunk_1.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_2.csv\n",
      "🔎 Detected rows: 81\n",
      "📤 Written to s3://pattern-detection-kanakb/output_detections/detection_1.csv\n",
      "📤 Detection file written: output_detections/detection_1.csv\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_2.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_3.csv\n",
      "📥 Processed: chunk_2.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_4.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_5.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_3.csv\n",
      "📥 Processed: chunk_3.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_6.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_7.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_4.csv\n",
      "📥 Processed: chunk_4.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_8.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_9.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_5.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_10.csv\n",
      "📥 Processed: chunk_5.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_11.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_6.csv\n",
      "📥 Processed: chunk_6.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_12.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_13.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_7.csv\n",
      "📥 Processed: chunk_7.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_14.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_15.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_8.csv\n",
      "📥 Processed: chunk_8.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_16.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_17.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_9.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_18.csv\n",
      "📥 Processed: chunk_9.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_19.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_10.csv\n",
      "📥 Processed: chunk_10.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_20.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_21.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_11.csv\n",
      "📥 Processed: chunk_11.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_22.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_23.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_12.csv\n",
      "📥 Processed: chunk_12.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_24.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_25.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_13.csv\n",
      "📥 Processed: chunk_13.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_26.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_27.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_14.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_28.csv\n",
      "📥 Processed: chunk_14.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_29.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_15.csv\n",
      "📥 Processed: chunk_15.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_30.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_31.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_32.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_33.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_34.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_35.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_16.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_36.csv\n",
      "📥 Processed: chunk_16.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_37.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_17.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_38.csv\n",
      "📥 Processed: chunk_17.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_39.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_18.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_40.csv\n",
      "📥 Processed: chunk_18.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_41.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_19.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_42.csv\n",
      "📥 Processed: chunk_19.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_43.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_20.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_44.csv\n",
      "📥 Processed: chunk_20.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_45.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_46.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_21.csv\n",
      "📥 Processed: chunk_21.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_47.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_22.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_48.csv\n",
      "📥 Processed: chunk_22.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_49.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_23.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_50.csv\n",
      "📥 Processed: chunk_23.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_51.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_24.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_52.csv\n",
      "📥 Processed: chunk_24.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_53.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_25.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_54.csv\n",
      "📥 Processed: chunk_25.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_55.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_56.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_26.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_57.csv\n",
      "📥 Processed: chunk_26.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_58.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_27.csv\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_59.csv\n",
      "📥 Processed: chunk_27.csv, rows = 10000\n",
      "✅ Uploaded: s3://pattern-detection-kanakb/transaction_chunks/chunk_60.csv\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_28.csv\n",
      "📥 Processed: chunk_28.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_29.csv\n",
      "📥 Processed: chunk_29.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_30.csv\n",
      "📥 Processed: chunk_30.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_31.csv\n",
      "📥 Processed: chunk_31.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_32.csv\n",
      "📥 Processed: chunk_32.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_33.csv\n",
      "📥 Processed: chunk_33.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_34.csv\n",
      "📥 Processed: chunk_34.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_35.csv\n",
      "📥 Processed: chunk_35.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_36.csv\n",
      "📥 Processed: chunk_36.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_37.csv\n",
      "📥 Processed: chunk_37.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_38.csv\n",
      "📥 Processed: chunk_38.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_39.csv\n",
      "📥 Processed: chunk_39.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_40.csv\n",
      "📥 Processed: chunk_40.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_41.csv\n",
      "📥 Processed: chunk_41.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_42.csv\n",
      "📥 Processed: chunk_42.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_43.csv\n",
      "📥 Processed: chunk_43.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_44.csv\n",
      "📥 Processed: chunk_44.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_45.csv\n",
      "📥 Processed: chunk_45.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_46.csv\n",
      "📥 Processed: chunk_46.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_47.csv\n",
      "📥 Processed: chunk_47.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_48.csv\n",
      "📥 Processed: chunk_48.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_49.csv\n",
      "📥 Processed: chunk_49.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_50.csv\n",
      "📥 Processed: chunk_50.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_51.csv\n",
      "📥 Processed: chunk_51.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_52.csv\n",
      "📥 Processed: chunk_52.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_53.csv\n",
      "📥 Processed: chunk_53.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_54.csv\n",
      "📥 Processed: chunk_54.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_55.csv\n",
      "📥 Processed: chunk_55.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_56.csv\n",
      "📥 Processed: chunk_56.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_57.csv\n",
      "📥 Processed: chunk_57.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_58.csv\n",
      "📥 Processed: chunk_58.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_59.csv\n",
      "📥 Processed: chunk_59.csv, rows = 10000\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "🔍 Looking for: s3://pattern-detection-kanakb/transaction_chunks/chunk_60.csv\n",
      "📥 Processed: chunk_60.csv, rows = 4643\n",
      "🔎 Detected rows: 81\n",
      "📊 Detections so far: 81\n",
      "📦 Running CHILD detection on full 594643 rows...\n",
      "Found CHILD Detections: 34\n",
      "📤 Written to s3://pattern-detection-kanakb/output_detections/detection_2.csv\n",
      "📤 Detection file written: output_detections/detection_2.csv\n",
      "📤 Written to s3://pattern-detection-kanakb/output_detections/detection_3.csv\n",
      "📤 Final detection file (less than 50 rows): output_detections/detection_3.csv\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "producer_thread = Thread(target=producer)\n",
    "consumer_thread = Thread(target=consumer)\n",
    "\n",
    "# Start threads\n",
    "producer_thread.start()\n",
    "consumer_thread.start()\n",
    "\n",
    "# Wait for both to finish\n",
    "producer_thread.join()\n",
    "consumer_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfdf35a-2ef8-4e0e-b6ef-0a1baf38b315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "updated_pattern_detection",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
