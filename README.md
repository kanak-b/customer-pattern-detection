# Customer Pattern Detection

This project simulates a real-time streaming data pipeline using **PySpark** and **AWS S3** to detect customer behavior patterns in merchant transactions.

---

## Architecture

![architecture](data/image.png)

### âš™ï¸ Mechanism X: Stream Uploader (`chunk_uploader.py`)

- Reads the full transactions file locally.
- Every **1 second**, slices the next **10,000 rows**, saves to `data/raw/`, and uploads to:
  ```
  s3://<S3_BUCKET>/transaction_chunks/chunk_0.csv, chunk_1.csv, ...
  ```

### âš™ï¸ Mechanism Y: Stream Detector (`stream_detection.py`)

- Monitors the same S3 prefix for new chunks.
- For each chunk:
  1. Reads the CSV file from S3.
  2. Converts it into a Spark DataFrame.
  3. Runs all 3 pattern detection rules.
  4. Writes detection results to:
     ```
     s3://<S3_BUCKET>/output_detections/detection_0.csv, ...
     ```
  - Detections are batched into chunks of 50 rows per file.

---

## Pattern Detection Rules

### Pattern 1 â€“ `UPGRADE`

- Customers in **top 10% by transaction count** for a merchant.
- Also in the **bottom 10% by average importance weight**.
- Only applies to merchants with **â‰¥ 50,000 transactions**.

### Pattern 2 â€“ `CHILD`

- Customers with:
  - **Avg transaction amount < â‚¹23**
  - **At least 80 transactions** with the merchant.

### Pattern 3 â€“ `DEI-NEEDED`

- Merchant has:
  - **More male customers than female**, but
  - **At least 100 female customers**
- Only includes customers with a **clear gender identity** (1 consistent gender per merchant).

---

## Detection Output Format

Each row in the detection output contains:

| Column        | Description                          |
| ------------- | ------------------------------------ |
| YStartTime    | Start time of chunk processing (IST) |
| detectionTime | Detection timestamp (IST)            |
| patternId     | `PatId1`, `PatId2`, or `PatId3`      |
| actionType    | `UPGRADE`, `CHILD`, or `DEI-NEEDED`  |
| customerName  | Customer ID (empty for DEI-NEEDED)   |
| merchantId    | Merchant ID                          |

---

## S3 Output Paths

| Type        | S3 Location                                         |
| ----------- | --------------------------------------------------- |
| Chunk Input | `s3://<bucket>/transaction_chunks/chunk_<i>.csv`    |
| Detections  | `s3://<bucket>/output_detections/detection_<i>.csv` |

---

## âš™ï¸ Setup Instructions

1. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

2. **Configure AWS credentials** in `.env`:
   ```
   AWS_ACCESS_KEY_ID=your_access_key
   AWS_SECRET_ACCESS_KEY=your_secret_key
   ```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw/                         # data from the gdrive
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ exploration.ipynb            # Jupyter notebooks for data exploration
â”œâ”€â”€ start_pipeline.py              # Main entry: starts streaming upload and detection
â”œâ”€â”€ config.py                      # Configuration: file paths, S3 info, chunk size
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pattern_runner.py          # Orchestrates all 3 pattern detections
â”‚   â””â”€â”€ patterns/
â”‚       â”œâ”€â”€ pat1_upgrade.py        # Pattern 1: UPGRADE
â”‚       â”œâ”€â”€ pat2_child.py          # Pattern 2: CHILD
â”‚       â””â”€â”€ pat3_dei_needed.py     # Pattern 3: DEI-NEEDED
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ gdrive.py                  # Downloads and loads CSVs from Google Drive
    â”œâ”€â”€ chunk_uploader.py          # Mechanism X: Uploads 10K chunks/sec to S3
    â””â”€â”€ stream_detection.py        # Mechanism Y: Consumes stream, runs detection


```

---
